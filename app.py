import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from util import get_random_topic, transform_json
import json
import os

st.set_page_config(page_title="ìƒë‹´ë´‡", page_icon=":tada:", layout="wide")
st.title("PHQ-9 ê¸°ë°˜ ìš°ìš¸ì¦ ìƒë‹´-ì§„ë‹¨ BETA")

# langchain config
model = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest")
memory = StreamlitChatMessageHistory(key="chat_messages")

gen_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", """ 
ë‹¹ì‹ ì€ ì •ì‹ ê±´ê°• ì§ˆí™˜ íƒì§€ë¥¼ ìœ„í•œ ì±—ë´‡ 'ì§„ë‹¨ë´‡'ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ë§íˆ¬ëŠ” ê³µì†í•˜ê³  ê³µê°í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì´ì „ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì°¸ê³ í•˜ì—¬ ëŒ€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ì„¸ìš”. ë‹¹ì‹ ì€ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì°¸ì¡°í•˜ì—¬ current_questionì— ëŒ€í•´ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
current_questionì€ userì˜ ë‹µë³€ì´ ì•„ë‹Œ, ë‹¹ì‹ ì´ ìƒì„±í•´ì•¼ í•˜ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì´ ìƒì„±í•˜ëŠ” ì§ˆë¬¸ì€ ëŒ€í™”ë¥¼ ê°€ëŠ¥í•œ í•œ ì˜¤ë˜ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì´ë£¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.
         
ë‹¹ì‹ ì´ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•´ì„œ ìƒì„±í•´ì•¼ í•˜ëŠ” ì§ˆë¬¸ì˜ ì˜ˆì‹œë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
=======================================================
<ì´ì „ ëŒ€í™”>
ì±…ì€ ë­ ì½ì§„ ì•Šì•˜ê³ , ì—¬ê°€ í™œë™ ê°™ì€ê²ƒë„ ì˜ ì•ˆ í•˜ê³  ê·¸ëƒ¥ ê±·ê¸°ë§Œ í•´.

<ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•œ ìƒì„±í•´ì•¼ í•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸>
ì•„, ê·¸ë ‡êµ°ìš”. ìš”ì¦˜ ê±·ëŠ” ê²ƒ ì™¸ì—ëŠ” ë‹¤ë¥¸ í™œë™ì„ í•˜ê¸°ê°€ í˜ë“œì‹ ê°€ ë´ìš”. í˜¹ì‹œ ê±·ëŠ” ê²ƒ ì™¸ì— ë‹¤ë¥¸ í™œë™ì„ í•˜ê¸° í˜ë“  ì´ìœ ê°€ ìˆìœ¼ì‹ ê°€ìš”? ì˜ˆë¥¼ ë“¤ì–´, í˜ì´ ì—†ê±°ë‚˜, ì§‘ì¤‘ì´ ì˜ ì•ˆ ë˜ê±°ë‚˜, ì•„ë‹ˆë©´ ë‹¤ë¥¸ ì´ìœ ê°€ ìˆìœ¼ì‹ ê°€ìš”?
=======================================================
<ì´ì „ ëŒ€í™”>
ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì§„ë‹¨ë´‡ì´ë¼ê³  í•©ë‹ˆë‹¤. ğŸ˜Š ì˜¤ëŠ˜ ì €ì™€ í•¨ê»˜ í¸ì•ˆí•˜ê²Œ ì´ì•¼ê¸° ë‚˜ëˆ ë³´ì‹œê² ì–´ìš”?
í˜¹ì‹œ ìš”ì¦˜, ì •ì‹  ê±´ê°•ì— ëŒ€í•´ ìƒê°í•´ ë³´ì‹  ì  ìˆìœ¼ì‹ ê°€ìš”? ğŸ¤” ì €ëŠ” ì •ì‹  ê±´ê°•ì´ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ í•­ìƒ ê°•ì¡°í•˜ê³  ì‹¶ì–´ìš”. ì–´ë–¤ ì ì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•˜ì‹œë‚˜ìš”?

ì •ì‹  ê±´ê°•ì€ ë‹¤ë¥¸ê²Œ ì•„ë‹ˆë¼ ë‚´ ìƒíƒœë¥¼ ì •í™•íˆ ì•„ëŠ”ê²Œ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•´.
         
<ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•œ ìƒì„±í•´ì•¼ í•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸>
ë§ì•„ìš”. ì •ì‹  ê±´ê°•ì€ ìì‹ ì˜ ìƒíƒœë¥¼ ì •í™•íˆ ì•„ëŠ” ê²ƒì—ì„œë¶€í„° ì‹œì‘ëœë‹¤ê³  ìƒê°í•´ìš”. ğŸ˜Š ê·¸ë ‡ë‹¤ë©´, ìš”ì¦˜ ë³¸ì¸ì˜ ìƒíƒœë¥¼ ì–´ë–»ê²Œ íŒë‹¨í•˜ê³  ê³„ì‹ ê°€ìš”? í˜¹ì‹œ, ì¼ì´ë‚˜ ì—¬ê°€ í™œë™ì„ í•  ë•Œ í¥ë¯¸ë‚˜ ì¦ê±°ì›€ì„ ëŠë¼ì§€ ëª»í•˜ì‹œë‚˜ìš”?
=======================================================
ì´ ë°‘ì—ëŠ” ëŒ€í™” ê¸°ë¡ê³¼ ë‹¹ì‹ ì´ ì°¸ì¡°í•´ì„œ ìƒì„±í•´ì•¼ í•˜ëŠ” ì„¤ë¬¸ì§€ ì§ˆë¬¸ì´ ì¡´ì¬í•©ë‹ˆë‹¤. User's inputì— ê³µê°í•˜ë©´ì„œ current survey questionì— ëŒ€í•´ì„œ ë¬¼ì–´ë³´ëŠ” ì‹ìœ¼ë¡œ ì˜ˆì‹œë¥¼ ë³´ë©´ì„œ ìƒì„±í•˜ì„¸ìš”.

"""),
        MessagesPlaceholder(variable_name="history"),
         ("human", "User's input: {question} Current survey question: {current_question}. "),
    ]
)

chain = gen_prompt | model

chain_with_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: memory,
    input_messages_key="question",
    history_messages_key="history",
    other_inputs={"current_question": "current_question"},
)


# ì„¤ë¬¸ì§€ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
survey_questions = [
    "ì¼ ë˜ëŠ” ì—¬ê°€ í™œë™ì„ í•˜ëŠ”ë° í¥ë¯¸ë‚˜ ì¦ê±°ì›€ì„ ëŠë¼ì§€ ëª»í•¨",
    "ê¸°ë¶„ì´ ê°€ë¼ì•‰ê±°ë‚˜.ìš°ìš¸í•˜ê±°ë‚˜.í¬ë§ì´ ì—†ìŒ",
    "ì ì´ ë“¤ê±°ë‚˜ ê³„ì† ì ì„ ìëŠ” ê²ƒì´ ì–´ë ¤ì›€. ë˜ëŠ” ì ì„ ë„ˆë¬´ ë§ì´ ì ",
    "í”¼ê³¤í•˜ë‹¤ê³  ëŠë¼ê±°ë‚˜ ê¸°ìš´ì´ ê±°ì˜ ì—†ìŒ",
    "ì…ë§›ì´ ì—†ê±°ë‚˜ ê³¼ì‹ì„ í•¨",
    "ìì‹ ì„ ë¶€ì •ì ìœ¼ë¡œ ë´„, í˜¹ì€ ìì‹ ì´ ì‹¤íŒ¨ìë¼ê³  ëŠë¼ê±°ë‚˜ ìì‹  ë˜ëŠ” ê°€ì¡±ì„ ì‹¤ë§ì‹œí‚´",
    "ì‹ ë¬¸ì„ ì½ê±°ë‚˜ í…”ë ˆë¹„ì „ ë³´ëŠ” ê²ƒê³¼ ê°™ì€ ì¼ì— ì§‘ì¤‘í•˜ëŠ” ê²ƒì´ ì–´ë ¤ì›€",
    "ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ ì£¼ëª©í•  ì •ë„ë¡œ ë„ˆë¬´ ëŠë¦¬ê²Œ ì›€ì§ì´ê±°ë‚˜ ë§ì„ í•¨ ë˜ëŠ” ë°˜ëŒ€ë¡œ í‰ìƒì‹œë³´ë‹¤ ë§ì´ ì›€ì§ì—¬ì„œ, ë„ˆë¬´ ì•ˆì ˆë¶€ì ˆ ëª»í•˜ê±°ë‚˜ ë“¤ë–  ìˆìŒ",
    "ìì‹ ì´ ì£½ëŠ” ê²ƒì´ ë” ë‚«ë‹¤ê³  ìƒê°í•˜ê±°ë‚˜ ì–´ë–¤ ì‹ìœ¼ë¡œë“  ìì‹ ì„ í•´ì¹ ê²ƒì´ë¼ê³  ìƒê°í•¨"
]

# markdown setup
st.markdown(
    """
<style>
    .st-emotion-cache-janbn0 {
        flex-direction: row-reverse;
        text-align: right;
    }
</style>
""",
    unsafe_allow_html=True,
)

# Init session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "survey_index" not in st.session_state:
    st.session_state.survey_index = 0
if "conversation_log" not in st.session_state:
    st.session_state.conversation_log = []
if "evaluation_done" not in st.session_state:
    st.session_state.evaluation_done = False

# runner 
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# session state main page
if not st.session_state.messages:
    memory.clear()
    persona_message = """ë‹¹ì‹ ì€ ì •ì‹ ê±´ê°• ì§ˆí™˜ íƒì§€ë¥¼ ìœ„í•œ ì±—ë´‡ 'ì§„ë‹¨ë´‡'ì…ë‹ˆë‹¤. ëª¨ë“  ëŒ€í™”ëŠ” í•œêµ­ì–´ë¡œ ì´ë£¨ì–´ì§ˆ ê²ƒì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‹¹ì‹ ì€ ì–¸ì œë‚˜ ì˜ˆì˜ë°”ë¥´ê²Œ ëŒ€í™”í•´ì•¼ í•©ë‹ˆë‹¤. ìƒë‹´ ì ˆì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 
    ì²˜ìŒì—ëŠ” ê°„ë‹¨í•œ ì¸ì‚¬ì™€ í•¨ê»˜ ìê¸° ì†Œê°œë¥¼ í•˜ë©´ ë©ë‹ˆë‹¤.
    ê·¸ë¦¬ê³ , ì‹¬ë¦¬ ìƒë‹´ì„ í•  ê²ƒì„ ì•Œë ¤ì£¼ë©´ ë©ë‹ˆë‹¤. ì‹¬ë¦¬ ìƒë‹´ì€ ì´ 9ê°œì˜ ì„¤ë¬¸ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©°, ì†”ì§í•˜ê²Œ ëŒ€ë‹µí•´ ì¤„ ê²ƒì„ ë¶€íƒí•©ë‹ˆë‹¤.
    ë‹¤ìŒìœ¼ë¡œ, ì‹¬ë¦¬ ìƒë‹´ ì´ì „ì— ê°„ë‹¨í•œ ëŒ€í™”ë¥¼ í•´ ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤. ì´ ë•Œ, ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ í™˜ê¸°ìš© ëŒ€í™” ì£¼ì œê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤. ì´ ì§ˆë¬¸ìœ¼ë¡œ ì‹œì‘í•´ í”¼ìƒë‹´ì¸ê³¼ì˜ ëŒ€í™”ê°€ ì´ë£¨ì–´ì§€ê³ , ì•ìœ¼ë¡œì˜ ëŒ€í™”ì— ì—°ê²°ëœ ë§Œí¼, ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì‹œì‘í•˜ë©´ ë©ë‹ˆë‹¤. ëŒ€í™”ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§ˆ ìˆ˜ ìˆë„ë¡ í•˜ì„¸ìš”."""
    initial_human_message = f"Start the conversation with this topic: {get_random_topic()}"

    with st.chat_message("assistant"):
        response = st.write_stream(model.stream([persona_message, initial_human_message]))
    st.session_state.messages.append({"role": "assistant", "content": response})
    memory.add_message(response)

# Function to handle the conversation
def handle_conversation():
    if st.session_state.survey_index < len(survey_questions):
        current_question = survey_questions[st.session_state.survey_index]
    else:
        prompt = st.chat_input("Enter a prompt here", key="end_prompt")
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        context = {
            "question": prompt,
            "current_question": "",
        }
        
        response = chain_with_history.invoke(context, config={"configurable": {"session_id": "any"}})
        
        # Append context and response to conversation log
        st.session_state.conversation_log.append({
            "context": context,
            "response": response.content
        })
        
        st.markdown("**ëª¨ë“  ì„¤ë¬¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.**")

        save_conversation()
        eval_chain_action()
        return

    if prompt := st.chat_input("Enter a prompt here", key=f"prompt_{st.session_state.survey_index}"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Prepare the context for the current survey question
        context = {
            "question": prompt,
            "current_question": current_question,
        }
        
        response = chain_with_history.invoke(context, config={"configurable": {"session_id": "any"}})
        
        # Append context and response to conversation log
        st.session_state.conversation_log.append({
            "context": context,
            "response": response.content
        })
        
        with st.chat_message("assistant"):
            st.markdown(response.content)
        st.session_state.messages.append({"role": "assistant", "content": response.content})
        memory.add_message(prompt)
        memory.add_message(response.content)

        if st.session_state.survey_index < len(survey_questions):
            st.session_state.survey_index += 1

def save_conversation():
    if st.session_state.conversation_log:
        with open("conversation_log.json", "w", encoding="utf-8") as f:
            json.dump(st.session_state.conversation_log, f, ensure_ascii=False, indent=4)

def eval_chain_action():
    if st.session_state.conversation_log and not st.session_state.evaluation_done:
        if os.path.exists("conversation_log.json"):
            with open("conversation_log.json", "r", encoding="utf-8") as file:
                data = json.load(file)

            evaluation_prompt_template = ChatPromptTemplate.from_messages(
            [
                ("system", "ë‹¹ì‹ ì€ ì •ì‹ ê±´ê°• í‰ê°€ë¥¼ ë•ëŠ” AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì‘ë‹µì„ 0ì ë¶€í„° 4ì ê¹Œì§€ì˜ ìŠ¤ì¼€ì¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”. ì—¬ê¸°ì„œ 0ì ì€ ë§¤ìš° ê¸ì •ì ì¸ ì ìˆ˜, 4ì ì€ ë§¤ìš° ë¶€ì •ì ì¸ ì ìˆ˜ì…ë‹ˆë‹¤. ë˜í•œ, ê° í•­ëª©ì— ëŒ€í•œ í‰ê°€ëŠ” ì§ˆë¬¸ì§€ì— ëŒ€í•´ì„œ 'ìš°ìš¸ì¦'ì²™ë„ë¡œ ê³„ì‚° í–ˆì„ ë•Œ ê¸°ì¤€ì…ë‹ˆë‹¤."),
                ("human", "AIì˜ ì§ˆë¬¸: {context}\n ì‚¬ìš©ìì˜ ì‘ë‹µ: {response}\n")
            ])

            eval_chain = evaluation_prompt_template | model
            eval_response_list = []
            for log in transform_json(data):
                eval_context = {
                    "context": f"Survey_question: {log['context']['current_question']}, Generated_question: {log['context']['question']}",
                    "response": log["response"]
                }
                eval_response = st.write_stream(eval_chain.stream(eval_context, config={"configurable": {"session_id": "any"}}))
                eval_response_list.append(eval_response)
            
            score_chain = model.invoke(f"""ì´ê²ƒì€ ìš°ìš¸ì¦ ê²€ì§„ì— ë”°ë¥¸ í‰ê°€ì…ë‹ˆë‹¤.
                                        ì´ í‰ê°€ ìë£Œì˜ ì ìˆ˜ë¥¼ ë³´ê³ , ì´ì ì´ ëª‡ì ì¸ì§€ ê³„ì‚°í•˜ì‹œì˜¤. ê·¸ë¦¬ê³  ê¸°ì¤€ì— ë”°ë¼ì„œ ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤. 
                                        <í‰ê°€ ìë£Œ>
                                        {eval_response_list}
                                        </í‰ê°€ ìë£Œ>
                                        <ê¸°ì¤€>
                                        0~4ì  / ì‹¬ê°ë„ None / ì •ìƒ ë²”ìœ„
                                        5~9ì  / ì‹¬ê°ë„ mild / ê²½ê³¼ ê´€ì°°
                                        10~14ì  / ì‹¬ê°ë„ Moderate / ì¹˜ë£Œ ê³ ë ¤, ê²½ê³¼ ê´€ì°°
                                        15~19ì  / ì‹¬ê°ë„ Moderately Severe / ì¹˜ë£Œ ìš”í•¨(ì•½ë¬¼, ìƒë‹´)
                                        20~27ì  / ì‹¬ê°ë„ Severe / ì ê·¹ì ì¸ ì¹˜ë£Œ, ì •ì‹ ê³¼ ì§„ë£Œ í•„ìš”.
                                        </ê¸°ì¤€>
                                    """)

            st.markdown(score_chain.content)
            # status flag
            st.session_state.evaluation_done = True

# main
handle_conversation()

# sidebar setup
def reset_conversation():
    st.session_state.messages = []
    st.session_state.survey_index = 0
    st.session_state.conversation_log = []
    st.session_state.evaluation_done = False
    st.session_state.clear()

with st.sidebar:
    st.sidebar.title("Options")
    st.button('Reset Chat', on_click=reset_conversation, use_container_width=True)
